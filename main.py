# ğŸ“œ main.py
# ğŸ’¥ ì´ íŒŒì¼ í•˜ë‚˜ë§Œ ì‹¤í–‰í•˜ë©´ ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì‘ë™í•©ë‹ˆë‹¤.
# (ğŸ”¥ MLflow, ì‹œë“œ ê³ ì •, Logging ê¸°ëŠ¥ì´ ëª¨ë‘ í†µí•©ë¨)

import os      # PYTHONHASHSEED ê³ ì • ë° MLflow URI ì„¤ì •
import random  # Python ê¸°ë³¸ random ì‹œë“œ ê³ ì •
import numpy as np # NumPy ì‹œë“œ ê³ ì •
import time
import argparse  # í„°ë¯¸ë„ ì¸ì íŒŒì‹±
import tempfile # Artifact ì €ì¥ì„ ìœ„í•œ ì„ì‹œ í´ë”
from omegaconf import OmegaConf, DictConfig # YAML ë° ì¸ì ë³‘í•©
import mlflow  # MLOps ì‹¤í—˜ ë¡œê¹…
import logging # (ğŸ”¥ ì‹ ê·œ) Tqdmê³¼ í˜¸í™˜ë˜ëŠ” ë¡œê¹…

# ğŸ­ í•µì‹¬ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜
from core_pipeline.run_pipeline import run_full_pipeline

# (ğŸ”¥ ì‹ ê·œ) main í•¨ìˆ˜ ë°–ì— ë¡œê±° ì„¤ì •
# (í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ëª¨ë“ˆì´ ì´ ì„¤ì •ì„ ìƒì†ë°›ì•„ ì‚¬ìš©)
logger = logging.getLogger(__name__)

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜:
    1. (ğŸ”¥ ì‹ ê·œ) ë¡œê¹…(Logging) ê¸°ë³¸ ì„¤ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    2. Argparseì™€ OmegaConfë¥¼ ì‚¬ìš©í•´ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    3. (ğŸ”¥ ì‹ ê·œ) ì¬í˜„ì„±ì„ ìœ„í•œ ê¸€ë¡œë²Œ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    4. MLflow ì‹¤í—˜(Run)ì„ ì‹œì‘í•˜ê³  Configë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
    5. core_pipelineì˜ run_full_pipeline í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
       (ë°˜í™˜ê°’: final_kpi_df, metrics)
    6. ë°˜í™˜ëœ Metricsì™€ Parquet íŒŒì¼ì„ MLflowì— ë¡œê¹…í•©ë‹ˆë‹¤.
    7. ì™„ë£Œ ë©”ì‹œì§€ ë° ì‹¤í–‰ ì‹œê°„ì„ ë¡œê¹…í•©ë‹ˆë‹¤.
    """

    # --- 1. (ğŸ”¥ ì‹ ê·œ) ë¡œê¹…(Logging) ì¤‘ì•™ ì„¤ì • ---
    # (ë‹¤ë¥¸ ëª¨ë“  ì‘ì—…ë³´ë‹¤ ë¨¼ì € ì‹¤í–‰)
    logging.basicConfig(
        level=logging.INFO, # INFO ë ˆë²¨ ì´ìƒë§Œ ì¶œë ¥
        format="[%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[
            logging.StreamHandler() # ì½˜ì†”(í„°ë¯¸ë„)ë¡œ ì¶œë ¥
            # (ì„ íƒ) íŒŒì¼ë¡œë„ ì €ì¥í•˜ë ¤ë©´ ì•„ë˜ í•¸ë“¤ëŸ¬ ì£¼ì„ í•´ì œ
            # logging.FileHandler("pipeline.log", mode='w') 
        ]
    )

    # --- 2. ì„¤ì • ë¡œë“œ (Argparse + OmegaConf) ---
    parser = argparse.ArgumentParser(description="EEG KPI Extraction Pipeline")
    parser.add_argument(
        '-c', '--config_path',
        type=str,
        default='./configs/base_config.yaml',
        help="Path to the base YAML config file."
    )
    args, unknown_args = parser.parse_known_args()

    # --- 3. ê¸°ë³¸ YAML ì„¤ì • ë¡œë“œ ---
    try:
        base_cfg = OmegaConf.load(args.config_path)
    except FileNotFoundError:
        # (ğŸ”¥ ìˆ˜ì •) print -> logger.error
        logger.error(f"âŒ ê¸°ë³¸ ì„¤ì • íŒŒì¼({args.config_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # --- 4. í„°ë¯¸ë„ ì¸ì(override) ë¡œë“œ ---
    cli_cfg = OmegaConf.from_cli(unknown_args)

    # --- 5. ì„¤ì • ë³‘í•© ---
    cfg = OmegaConf.merge(base_cfg, cli_cfg)
    
    # --- 6. (ğŸ”¥ ì‹ ê·œ) ì¬í˜„ì„±ì„ ìœ„í•œ ê¸€ë¡œë²Œ ì‹œë“œ ê³ ì • ---
    try:
        seed = cfg.GLOBAL_RANDOM_SEED
        os.environ['PYTHONHASHSEED'] = str(seed)
        random.seed(seed)
        np.random.seed(seed)
        # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
        logger.info(f"ğŸ§¬ [INFO] Global random seedë¥¼ {seed}ë¡œ ê³ ì •í•©ë‹ˆë‹¤.")

        # (ì£¼ì„: í–¥í›„ PyTorch ì‚¬ìš© ì‹œ)
        # try:
        #     import torch
        #     torch.manual_seed(seed)
        #     if torch.cuda.is_available():
        #         torch.cuda.manual_seed_all(seed)
        #         torch.use_deterministic_algorithms(True)
        #         torch.backends.cudnn.deterministic = True
        #         torch.backends.cudnn.benchmark = False
        # except ImportError:
        #     pass # PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ

    except Exception as e:
        # (ğŸ”¥ ìˆ˜ì •) print -> logger.warning
        logger.warning(f"[WARN] ì‹œë“œ ê³ ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ (configì— GLOBAL_RANDOM_SEEDê°€ ì—†ëŠ”ì§€ í™•ì¸): {e}")

    # --- 7. (ğŸ”¥ ì‹ ê·œ) MLflow ì„¤ì • ë° ì‹¤í—˜ ì‹œì‘ ---
    mlflow.set_tracking_uri(f"file:{os.path.abspath('mlruns')}")
    experiment_name = cfg.get("EXPERIMENT_NAME", "EEG_KPI_Analysis")
    mlflow.set_experiment(experiment_name)

    # MLflow ì‹¤í—˜(Run) ì‹œì‘
    with mlflow.start_run() as run:
        run_id = run.info.run_id
        # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
        logger.info(f"ğŸš€ MLflow ì‹¤í—˜ ì‹œì‘. Run ID: {run_id}")
        
        # --- 8. (ğŸ”¥ ì‹ ê·œ) Config ë¡œê¹… ---
        try:
            cfg_dict = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)
            mlflow.log_params(cfg_dict)
            # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
            logger.info(f"    MLflow: Config íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ.")
        except Exception as e:
            # (ğŸ”¥ ìˆ˜ì •) print -> logger.warning
            logger.warning(f"[WARN] MLflow Config ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        
        # --- 9. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
        # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
        logger.info("="*70)
        logger.info("ğŸ§  EEG KPI ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        logger.info(f"â–¶ï¸ ê¸°ë³¸ ì„¤ì • íŒŒì¼: {args.config_path}")
        if unknown_args:
            logger.info(f"â–¶ï¸ ëŸ°íƒ€ì„ ì„¤ì • (Override): {unknown_args}")
        logger.info(f"â–¶ï¸ MLflow ì‹¤í—˜ëª…: {experiment_name}")
        logger.info("="*70)

        start_time = time.time()  # ì‹œì‘ ì‹œê°„ ê¸°ë¡

        try:
            # (ğŸ”¥ ìˆ˜ì •) run_full_pipelineì´ (df, metrics)ë¥¼ ë°˜í™˜
            final_kpi_df, metrics = run_full_pipeline(cfg=cfg)

            if final_kpi_df is None:
                # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
                logger.info("\n[INFO] ì²˜ë¦¬ëœ ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                mlflow.log_param("status", "no_valid_data")
                return

            end_time = time.time()  # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
            total_time = end_time - start_time

            # --- 10. (ğŸ”¥ ì‹ ê·œ) Metrics ë¡œê¹… (metrics.json ëŒ€ì²´) ---
            # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
            logger.info(f"    MLflow: Metrics ë¡œê¹… ì¤‘...")
            if metrics:
                mlflow.log_metrics(metrics)
            
            mlflow.log_metric("pipeline_duration_sec", total_time)
            mlflow.log_metric("total_epochs_processed", len(final_kpi_df))
            mlflow.log_metric("total_kpis_generated", len(final_kpi_df.columns))

            # --- 11. (ğŸ”¥ ì‹ ê·œ) Artifact (Parquet) ë¡œê¹… (features.parquet ëŒ€ì²´) ---
            # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
            logger.info(f"    MLflow: Artifact (features.parquet) ë¡œê¹… ì¤‘...")
            with tempfile.TemporaryDirectory() as tmpdir:
                parquet_path = os.path.join(tmpdir, "features.parquet")
                final_kpi_df.to_parquet(parquet_path, index=False)
                mlflow.log_artifact(parquet_path, artifact_path="features")

            # (ğŸ”¥ ìˆ˜ì •) print -> logger.info
            logger.info("\n" + "="*70)
            logger.info(f"âœ… íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f} ì´ˆ")
            logger.info(f"ğŸ“Š MLflow UIì—ì„œ Run ID '{run_id}'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            logger.info("="*70)

        except Exception as e:
            # (ğŸ”¥ ìˆ˜ì •) print -> logger.critical
            logger.critical("\n" + "!"*70)
            logger.critical(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.critical(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
            mlflow.log_param("status", "pipeline_failed")
            mlflow.log_text(str(e), "error_details.txt")
            import traceback
            # (ğŸ”¥ ìˆ˜ì •) traceback.print_exc() -> logger.error()
            logger.error(traceback.format_exc())
            logger.critical("!"*70)


if __name__ == "__main__":
    # ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë˜ì—ˆì„ ë•Œë§Œ main() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    main()